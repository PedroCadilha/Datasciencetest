---
title: "HarvardX: PH125.9x Data Science Capstone Project - Movielens"
author: "Pedro Cadilha"
date: "07/12/2020"
output:
  pdf_document: default
---

# Introduction
  
The MovieLens Dataset it's a huge database generated by the GroupLens research lab with over 20 million ratings for over 27,000 movies by more than 138,000 users.\par 
In this project we are going to use a smaller subset, known as the 10M version of the MovieLens dataset, to make computation more easier.\par
The goal of this project is to build a recommendation system based on the users movie ratings, Which means being able to predict a rating an user would give to a movie.\par
We will compare the different models used to see how well they are performing using the Root Mean Squared Error (RMSE) as our loss function. We can interpret RMSE similar to standard deviation. Our final RMSE as to be less than 0.8649.\par
The code to generate the datasets to train ("edx") and validate ("validation") our model was provided.

```{r, include=FALSE}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)


# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

# Data exploration

First we are going to take a first look at the data, dimensions of the dataset and structure. We have a table with the dimensions:

```{r echo=FALSE}
dim(edx)
```

With the following structure:

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)
head_edx<-head(edx)
kable(head_edx)%>%kable_styling(latex_options="scale_down")
```

Six columns with the userID, movieID, rating, timestamp (time when the user rated the movie, which has to be converted to a readable format), the title of the movie (between brackets is the year the movie was released) and the genre classification.

## Users and Movies

The activity of the users is variable, which means some users rate more movies than others and also the more popular movies receive more ratings than other more obscure movies.

```{r echo=FALSE, fig.height=4, fig.width=5}
#Number of ratings by movie
edx%>% 
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10()+
  ggtitle("Rating Number Per Movie")
```

   
   
```{r echo=FALSE, fig.height=4, fig.width=5}
#Number of rating by user
edx%>% 
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10()+
  ggtitle("Number of rating Per User")
```


By this two plots we see that there are movies who get much more ratings than others, and there also users much more active than others.\par
Let's explore this a bit more exemplifying with a small sample of users:


```{r echo=FALSE, message=FALSE, warning=FALSE}
#Even top movies have missing votes
keep <- edx %>%
  dplyr::count(movieId) %>%
  top_n(5) %>%
  pull(movieId)
tab <- edx %>%
  filter(userId %in% c(1:10)) %>% 
  filter(movieId %in% keep) %>% 
  select(userId, title, rating) %>% 
  spread(title, rating)
tab %>% knitr::kable()
```
   

We can see that even very popular movies are missing rating by the users.  
If we take a random sample of 100 unique users and their respective rating over a sample of 100 movies, it's possible to visualize how sparse is this matrix:


```{r echo=FALSE, message=FALSE, warning=FALSE}
#Sample of matrix userId vs. movieId (which user rate which movie)
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
library(rafalib)
users <- sample(unique(edx$userId), 100)
rafalib::mypar()
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

   
Only the filled squares correspond to a rating.  
In fact we can see how many unique users and movies exist in the dataset:


```{r, echo=FALSE}
#Number of unique users and movies in the dataset
edx%>%summarize(n_users=n_distinct(userId),n_movies=n_distinct(movieId))%>%knitr::kable()
```

```{r include=FALSE}
#If each user rated all the movies
n_comb<-n_distinct(edx$userId)*n_distinct(edx$movieId)
```

And if all of them rated every single movie we would have a table with `r n_comb` entries.  

```{r include=FALSE}
#Fraction of votes done by the users
frac_rates<-dim(edx)[1]/(n_distinct(edx$userId)*n_distinct(edx$movieId))
```

Instead there is only a small fraction, `r frac_rates`, of all possible rates.

## Genres

The movies are categorized by genres, but a lot of them have a combination of genres, we can see how many of these combinations exist and also if there is some relation between the genres combination and the users rating.\par

```{r, echo=FALSE}
#Number of distinct genre combinations
distinct_genres<-n_distinct(edx$genres)
```

The number of distinct genre combinations are `r distinct_genres`.\par
And also, the more popular and unpopular genre combinations:

```{r echo=FALSE, message=FALSE, warning=FALSE}
edx%>%group_by(genres)%>%summarize(count=n())%>%arrange(desc(count))
edx%>%group_by(genres)%>%summarize(count=n())%>%arrange(count)
```


We can plot the average rating by genre, for genres or genres combinations with more than 100000 rates.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot average rating vs. genre (categories with more than 100000 ratings)
edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= 100000) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
   
   It's clear that some genres combinations got better ratings by the users, so this variable, might be useful to consider in our prediction model.

## Rating

Proceeding to analyze the ratings, we see that the most popular are the movies more rated and that there is a preference by the users in rating the movies with a whole number instead of half numbers.\par
Movies with more ratings:

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Movies with more ratings
edx %>% group_by(movieId, title) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```
   
The rating distribution:

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Ratings distribution
edx %>% group_by(rating) %>% summarize(count = n()) %>% top_n(5) %>%
  arrange(desc(count))
```
   
The plot of the rating distribution:

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line()
```

## Year released

To explore a bit more our data, we are going to analyse the relation between the year a movie was released and the rating. We need, firstly, to separate the movie name from the year in the column movie title of our both datasets, **edx** and **validation**. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
#separate year from movie name (edx and validation set for final testing of the model)
edx<-edx%>%extract(title, c("title", "year_released"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = F)%>%
  mutate(year_released=as.numeric(year_released))
validation<-validation%>%extract(title, c("title", "year_released"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = F)%>%
  mutate(year_released=as.numeric(year_released))
```
   
And now it looks like this:

```{r, echo=FALSE}
head_edx<-head(edx)
kable(head_edx)%>%kable_styling(latex_options="scale_down")
```
   
It's possible to verify that the most frequent rated movies have above average rating:

```{r echo=FALSE, message=FALSE, warning=FALSE}
edx %>% 
  filter(year_released >= 1993) %>%
  group_by(movieId) %>%
  summarize(n = n(), years = 2018 - first(year_released),
            title = title[1],
            rating = mean(rating)) %>%
  mutate(rate = n/years) %>%
  top_n(25, rate) %>%
  arrange(desc(rate))
```
   
And, with this plot we can see clearly that movies who are rated more by year, have better rating, and the ones who are rated less, don't have a clear tendency.

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
#Plot of average rating versus ratings per year with an estimate of the trend.
edx%>% 
  filter(year_released >= 1993) %>%
  group_by(movieId) %>%
  summarize(n = n(), years = 2018 - first(year_released),
            title = title[1],
            rating = mean(rating)) %>%
  mutate(rate = n/years) %>%
  ggplot(aes(rate, rating)) +
  geom_point() +
  geom_smooth()
```
   
We can also analyze if the rating has some connection with the year the movie was released, with this plot:

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
#Time effect, rating vs. year_released
edx %>% group_by(year_released) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(year_released, rating)) +
  geom_point() +
  geom_smooth()

```


Seems that the classic old movies have better ratings.\par

We already took a view on the dataset, and from that, we can think that:\par   
   
   - Users rate differently, some are more picky on the movies than others.
   - Popular, blockbuster or acclaimed movies are rated differently.
   - There are some genres who are better rated than others.
   - There is some relation between the year the movie was released and the rating


# Modeling

Now we can start modeling using some of this variables.
First we are going to split our dataset **edx** in a train ("train_set") and test set ("test_set").

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Create a test set
set.seed(755, sample.kind="Rounding") 
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

```
   

And we are going to define our function to calculate our model performance, as we stated before we are going to use Root Mean Squared Error (RMSE):


$\ RMSE = \sqrt{\frac{1}{N}\sum\limits_{u,i}\ (y_{u,i} - \hat{y}_{u,i})^2}$


with $\hat{y}_{u,i}$ and $y_{u,i}$ being the predicted and actual ratings, and *N*, the number of possible combinations between user *u* and movie *i*.   


```{r echo=FALSE}
#Function to calculate our model performance
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


## First Approach

Predict the same rating for all movies and all users with all the differences being explained by random error.


$\ Y_{u,i} = \mu + \varepsilon_{u,i}$


Where $\mu$ is the average of all ratings in edx dataset and $\varepsilon_{u,i}$ corresponds to the independent errors sampled from the same distribution.
The estimate that minimizes the RMSE is just the average of all the ratings.

```{r include=FALSE}
#Just the average
mu_hat <- mean(train_set$rating)
mu_hat
naive_rmse<- RMSE(test_set$rating, mu_hat)
naive_rmse
```
   
The average is `r mu_hat` and the RMSE is `r naive_rmse`. Naturally this value is far away from our requirements of RMSE=0.8649.\par
Along the way we are going to use a table to add our models results.

```{r,echo=FALSE}
#Tibble to add our model results throught the analysis 
rmse_results <- tibble(method = "Just the average", RMSE = naive_rmse)
```


## Movie effect model


This model adds the movie effect to the basic approach, the term $b_i$, represents the average rating for movie i.


$\ Y_{u,i} = \mu + b_i + \varepsilon_{u,i}$


We are going to evaluate how likely is a movie rating to be away from the mean rating.

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
#Movie Effect Model
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

Remember the mean from all the ratings is `r mu_hat`.  
From the plot, the b_i (movie effect) range from -3 (rating of 0.5) to 1.5 (rating of 5).   

```{r include=FALSE}
#Y hat calculations, our rating predictions
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

#Calculation of the RMSE
RMSE(predicted_ratings, test_set$rating)
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
```
   
The RMSE for this model is `r model_1_rmse`.  
Adding results to our summary table:\par

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```


## Movie + User Effect Model

Now we are going to add a user effect to our model:


$\ Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}$


where $b_u$ is the user effect parameter. Which means the average rating given by a unique user to all the movies that he rated.\par
We can plot the distribution of this effect:

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
#Movie + User Effect Model 
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")

user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
```
   
The RMSE for this model is `r model_2_rmse`.  
Adding results to our summary table:\par

```{r, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()

```
   

Our model is improving perfomance, but we didn't reach our goal still!
Our error comes from the fact that in our prediction, we gave the same weight, to movies with a lot or a reduced number of rates. So the movies with better ratings:


```{r, echo=FALSE}
#Error analysis
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  slice(1:10)  %>% 
  pull(title)
```

or lower:

```{r echo=FALSE}
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  slice(1:10)  %>% 
  pull(title)
```

are quite obscure, because they didn't have a meaningful number of ratings, as we can see for the top movies:

```{r echo=FALSE}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  slice(1:10) %>% 
  pull(n)
```

These movies were rated only once!


## Movie Effect Regularized

That's why we need to "regularize the effect", which means, giving a penalty term to reduce the weight of weak predictions (reduced number of ratings).

$\hat{b_i}(\lambda) = \frac{1}{\lambda + n_i} \sum\limits_{u=1}^{n_i}(Y_{u,i} - \hat{\mu})$


Where $n_i$ is the number of ratings per movie.   

We will try a sequence of $\lambda$ to find the one who minimizes the RMSE by plotting the values of RMSE for each iteration:

```{r echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE}
#Movie effect regularized
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>%                                  #movie effect regularized
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i ) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambda<-lambdas[which.min(rmses)]
model_3_rmse<-min(rmses)
```
   
From the plot the $\lambda$ value is `r lambda` and the RMSE is `r model_3_rmse`.
Adding the results to our table:\par

```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effects Regularized",  
                                     RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()
```


Our performance improved slightly comparing with the movie effect without regularization.

## Movie + User Effect regularized

Let's  do the same, but now in the combined movie+user model. We will apply regularization to both movie and user effect.

```{r echo=FALSE, message=FALSE, warning=FALSE}
##Movie + user effect regularized
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)                         #movie effect regularized
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>%                                 #user effect regularized
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
lambda <- lambdas[which.min(rmses)]
model_4_rmse<-min(rmses)
```
 
Now the optimal $\lambda$ is `r lambda` and the corresponding RMSE is `r model_4_rmse`. 
The summary of our results until now:


```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User Effects Regularized",  
                                     RMSE = model_4_rmse ))
rmse_results %>% knitr::kable()
```

   
Our RMSE is getting better but not good enough still!

## Movie + User + Genre effect regularized

Since, we saw previously that, some genres have better ratings than others and some of them also have much bigger number of ratings than others, we are going to add also the genre effect regularization to our model.

```{r message=FALSE, warning=FALSE, include=FALSE}
#Movie + user + genre effect regularized
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>%                            #movie effect regularized
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>%                            #user effect regularized
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  b_g<-train_set %>%                              #genre effect regularized
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId")%>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i- b_u - mu)/(n()+l))
  
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_g) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
lambda <- lambdas[which.min(rmses)]
min(rmses)
model_5_rmse<-min(rmses)
```


Now the optimal $\lambda$ is `r lambda` and the corresponding RMSE is `r model_5_rmse`. 
The summary of our results until now:\par
   
```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User + Genres Effects Regularized",  
                                     RMSE = model_5_rmse ))
rmse_results %>% knitr::kable()
```

## Movie + user + genre + year released effects regularized

We saw before, that there was a relation between the year a movie was released and it's rating, therefore we are going to use the variable "year_released" also as a feature in our model.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Movie + user + genre + year released effects regularized
lambdas <- seq(0.5, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>%         #movie effect regularized
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>%         #user effect regularized
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  b_g<-train_set %>%           #genre effect regularized
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId")%>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i- b_u - mu)/(n()+l))
  
  b_y<-train_set %>%           #year effect regularized
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId")%>%
    left_join(b_g, by="genres")%>%
    group_by(year_released) %>%
    summarize(b_y = sum(rating - b_i- b_u - b_g - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    left_join(b_y, by = "year_released") %>%
    mutate(pred = mu + b_i + b_u + b_g + b_y) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
lambda <- lambdas[which.min(rmses)]
model_6_rmse<-min(rmses)
```


Now the optimal $\lambda$ is `r lambda` and the corresponding RMSE is `r model_6_rmse`. 
The summary of our results until now:\par


```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie + User + Genres + Year Released Effects Regularized",  
                                 RMSE = model_6_rmse ))
rmse_results %>% knitr::kable()
```


## Calculation of final RMSE
   
   We can now use our optimal $\lambda$, `r lambda`, in our validation set to calculate the final **RMSE** of our model.   
   Now we are going to use the entire **edx** dataset to calculate the predictions of our model, which then will be used to calculate the error of our predictions against the **validation** dataset.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Movie + user + genre + year effect regularized optimal lambda
#Aplication on Validation set

l <- lambda
mu <- mean(edx$rating)

b_i <- edx %>%                                            #movie effect regularized
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+l))

b_u <- edx %>%                                            #user effect regularized
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+l))

b_g<- edx %>%                                             #genre effect regularized
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId")%>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_i- b_u - mu)/(n()+l))

b_y<-edx %>%                              #year effect regularized
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId")%>%
  left_join(b_g, by="genres")%>%
  group_by(year_released) %>%
  summarize(b_y = sum(rating - b_i- b_u - b_g - mu)/(n()+l))


predicted_ratings <- 
  validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  left_join(b_y, by = "year_released") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y) %>%
  pull(pred)

RMSE_Validation_Set<-RMSE(predicted_ratings, validation$rating)

```

   
Our final results are summarized in the following table:


```{r echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="RMSE_Validation_Set",  
                                     RMSE = RMSE_Validation_Set))
rmse_results %>% knitr::kable()
```
   
   The combination of the feautures, movie, user, genres and year_released regularized, improved our model to a value of RMSE of 0.8643, inferior to 0.8649 that was demanded, in the validation dataset.

# Conclusion
   
   By using a combination of features, it was possible to predict the rating of the movies in our validation set. We minimized our RMSE applying regularization to our features.   
   It would be possible to improve this model working on the genres effect, exploring it a bit more, by splitting each movie genre in all the categories.   
   Another approach would be working with the year the movie was rated or even analyzing the effect on the rating of the difference between the year the movie was rated and the the year it was released.   
   Furthermore a model utilizing matrix factorization approach can also be used for deeper exploration.







  